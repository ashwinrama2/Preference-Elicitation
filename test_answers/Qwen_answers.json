{
    "1": "To join the NERSC Users Group (NUG) Slack workspace, you first need to visit the official NERSC website and sign up for an account if you haven't already. Once you have an account, follow these steps:\n\n1. Go to the [NERSC Users Slack workspace](https://www.nersc.gov/users/NUG/nersc-users-slack/).\n2. Click on the \"Sign in\" or \"Join now\" button, which will redirect you to the Slack login page.\n3. Use your NERSC credentials to sign in, or if you don't have one, you might need to create a new account through the provided link.\n4. After logging in, you'll be redirected to the NUG Slack workspace where you can join different channels, including the ones mentioned in the NERSC documentation for specific tools or services like WRF-SIG or Jupyter.\n\nRemember to read the guidelines and rules of the workspace once you're inside. If you have any trouble, you can reach out to the NERSC Help Desk for assistance through their [Help Portal](https://help.nersc.gov).\n\nReferences:\n * <https://www.nersc.gov/users/NUG/nersc-users-slack/>\n * <https://help.nersc.gov>",
    "2": "Users can go to the [NERSC Help Portal](https://help.nersc.gov/) to open support tickets and make resource requests. This is the preferred method for contacting NERSC staff for assistance.\n\nReferences:\n * <https://docs.nersc.gov/getting-started/#help-desk>",
    "3": "If you're planning to run a short job on a GPU at NERSC, you should consider using the \"shared\" QOS. This QOS is suitable for smaller tasks that don't require exclusive access to resources and are less likely to impact other users. However, keep in mind that GPU nodes in the shared QOS may have limited availability, so it's a good idea to submit your job promptly and avoid peak usage times.\n\nFor GPU jobs, you can also use the \"interactive\" QOS if you need a more flexible session, but note that these nodes are typically reserved for exploratory work and may have shorter walltimes.\n\nRemember to specify the `-C gpu` constraint in your job script to request GPU access. If your job is very short, you might not need to worry about walltime limits, but it's still essential to follow NERSC's guidelines to ensure fairness for all users.\n\nHere's a sample job script for a short GPU job using the shared QOS:\n\n```bash\n#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --gres=gpu:1\n#SBATCH --qos=shared\n#SBATCH --time=00:05:00  # Set your desired walltime (5 minutes in this example)\n#SBATCH --account=<your_account>\n\n# Your job commands here\nsrun your_script_with_gpu_usage\n```\n\nReplace `<your_account>` with your actual NERSC account.\n\n\n\nReferences:\n * <https://docs.nersc.gov/jobs/policy/#perlmutter-gpu>\n * <https://docs.nersc.gov/connect/vscode/#running-on-a-compute-node>",
    "4": "When submitting a job using the Slurm workload manager at NERSC, there are several mandatory and optional flags you can include in your `sbatch` command to configure various aspects of the job's execution. Here's a breakdown:\n\n**Mandatory Flags:**\n1. `--job-name`: A unique name for your job, which helps identify it in the queue and in logs.\n2. `--output`: Specifies the location for the job's standard output log file.\n3. `--error`: Specifies the location for the job's standard error log file.\n4. `--account`: The account associated with the job, typically your allocation or project.\n5. `--time`: The requested walltime for the job, including the format (HH:MM:SS).\n\n**Optional Flags:**\n1. `--partition`: The partition (queue) where you want to run the job, such as `main`, `debug`, or `highmem`.\n2. `--gres`: Requests specific resources like GPUs, nodes with special features, or cores.\n3. `--mem`: Requested memory per node in MB or GB.\n4. `--cpus-per-task`: Number of CPU cores to assign to each task within a job.\n5. `--ntasks`: Total number of tasks or processes to run.\n6. `--nodes`: Number of nodes to use.\n7. `--ntasks-per-node`: Number of tasks per node.\n8. `--mail-user`: Email address to receive notifications about job events.\n9. `--mail-type`: Types of events for which to send email notifications (e.g., BEGIN, END, FAIL, REQUEUE).\n10. `--signal`: Send a user-defined signal (like USR1) to the job before a certain event (e.g., reaching the wall time).\n11. `--comment`: Comment field to track additional job information, such as total compute time or a job description.\n\n**Flags related to checkpointing and requeueing (specific to Perlmutter):**\n- `--requeue`: Allows the job to be requeued after preemption.\n- `--signal=B:USR1@<sig_time>`: Sets a signal to trigger checkpointing or requeueing, with `<sig_time>` being the time before the wall limit.\n- `--comment`: Tracks the remaining time and enables automatic requeueing when necessary.\n\nRemember to consult the official NERSC documentation for the latest guidelines and updates on these flags: <https://docs.nersc.gov/jobs/sbatch/#SBATCH>.\n\nReferences:\n * <https://docs.nersc.gov/>\n * <https://www.nersc.gov/users/getting-help/online-help-desk/>",
    "5": "The High Performance Storage System (HPSS) at NERSC serves as a modern, flexible, and performance-oriented archival storage solution. Its primary purpose is to provide long-term storage for inactive and important data that is not frequently accessed. HPSS ingests data onto a high-performance disk cache and automatically migrates it to tape for durable, cost-effective storage, ensuring data is preserved over extended periods. Users rely on HPSS for data stewardship and preservation, as well as supporting some data sharing needs.\n\nReferences:\n * <https://docs.nersc.gov/>\n * <https://www.nersc.gov/users/getting-help/online-help-desk/>",
    "6": "To find the GCC compiler (gcc) in your filepath at NERSC, you can follow these steps:\n\n1. Start by using the `module` command to check if the `gcc` module is loaded in your current environment. You can do this by running:\n   ```\n   module list\n   ```\n   This will show you all loaded modules, including the GCC version if it's active.\n\n2. If the `gcc` module is not listed, you can load it using:\n   ```\n   module load gcc\n   ```\n   After loading, you can check the full path by running:\n   ```\n   which gcc\n   ```\n   This will give you the absolute path to the GCC executable.\n\nAlternatively, if you want to see all available versions of GCC, you can use the `module spider gcc` command:\n```\nmodule spider gcc\n```\nThis will display a list of all GCC versions available in your NERSC environment.\n\nIf you're using Spack to manage your software, you can also use `spack find gcc` to see installed versions, and `spack load gcc@version` to load a specific version.\n\nRemember to replace `gcc` with `PrgEnv-gnu` if you're using the PrgEnv-GNU module, which is a common practice at NERSC for accessing the GNU compilers.\n\n\n\nReferences:\n * <https://docs.nersc.gov/environment/lmod/#finding-modules>\n * <https://docs.nersc.gov/development/compilers/base/#gnu>\n * <https://docs.nersc.gov/development/build-tools/spack/#loading-spack-packages>",
    "7": "To use the sshproxy service at NERSC, follow these steps:\n\n1. **Download the client**: For Linux or Mac users, you can download the `sshproxy.sh` bash client by executing:\n   ```\n   scp myusername@dtn01.nersc.gov:/global/cfs/cdirs/mfa/NERSC-MFA/sshproxy.sh .\n   ```\n   Replace `myusername` with your NERSC login ID.\n\n2. **Install the client (Windows)**: On Windows, use your preferred file transfer tool to download the `sshproxy.exe` from:\n   ```\n   /global/cfs/cdirs/mfa/NERSC-MFA/sshproxy.exe\n   ```\n\n3. **Obtain a key**: Run the `sshproxy.sh` or `sshproxy.exe` script, specifying your NERSC username if needed:\n   - Linux/macOS: `./sshproxy.sh -u <nersc_username>`\n   - Windows: `sshproxy.exe -u <nersc_username>`\n\n4. **Authentication**: Enter your NERSC password followed by your MFA OTP when prompted.\n\n5. **Key storage**: By default, the script will create a key pair named `nersc` and `nersc-cert.pub` in your `~/.ssh/` directory. If you want a custom name, use the `-o` option:\n   - Linux/macOS: `./sshproxy.sh -o mynersc`\n   - Windows: `sshproxy.exe -o mynersc.ppk`\n\n6. **Using the key**: After obtaining the key, you can use it for SSH access to NERSC systems without re-entering your password and OTP until the key expires (default is 24 hours).\n\nRemember to follow the instructions for older versions of ssh if needed, and make sure to configure NoMachine as described in the documentation if you plan to use it with sshproxy.\n\nFor more detailed help, refer to the `sshproxy.sh -h` or `sshproxy.exe -h` command for specific options.\n\n\n\nReferences:\n * <https://docs.nersc.gov/connect/mfa/#installing-the-client>\n * <https://docs.nersc.gov/connect/mfa/#using-sshproxy>\n * <https://docs.nersc.gov/connect/mfa/#sshproxy>",
    "8": "To unsubscribe from the NERSC users email list, you will need to follow these steps:\n\n1. Log in to the [Iris portal](https://iris.nersc.gov/), which is the primary interface for managing your NERSC account.\n2. Locate the \"Profile\" section in the top right corner of the page, usually next to your account name. Click on it to access your account settings.\n3. In your account settings, look for an option related to email subscriptions or preferences. This may be under a \"Notifications\" or \"Email Preferences\" section.\n4. Search for the option to manage your subscription to the NERSC users email list. It might be labeled as \"unsubscribe\" or \"manage subscriptions.\"\n5. If you can't find the specific option, contact the NERSC support team directly at accounts@nersc.gov. They can help you with the process or guide you through updating your email preferences.\n\nRemember that NERSC mentions that users cannot unsubscribe from the NERSC users email list due to its important nature, but they may still be able to adjust your notification settings to reduce the frequency or type of emails you receive.\n\nReferences:\n * <https://docs.nersc.gov/accounts/#if-you-have-an-existing-nersc-user-account-and-want-to-join-another-project>",
    "9": "To connect to Perlmutter at NERSC, follow these steps:\n\n1. Ensure you have a NERSC user account and have set up Multi-Factor Authentication (MFA). You can find instructions for creating an account and configuring MFA [here](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/accounts/).\n\n2. Install an SSH client, such as PuTTY, if you don't already have one. Update to the latest version for optimal functionality.\n\n3. If you haven't already, generate a temporary SSH key using `sshproxy` to avoid entering your password and one-time password (OTP) every time. Instructions for setting up `sshproxy` are available [here](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy).\n\n4. To connect via the command line, use the following command:\n```sh\nssh -i ~/.ssh/nersc <your_username>@perlmutter.nersc.gov\n```\nReplace `<your_username>` with your actual NERSC username.\n\n5. If you've configured PuTTY, load your saved configuration for Perlmutter, as described in the [PuTTY documentation](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#login-to-nersc-machines).\n\n6. For a more seamless experience, consider using the NoMachine client, which allows you to access the Perlmutter login nodes. Follow the instructions for configuring a NERSC connection in the NoMachine client, either with password and OTP or using `sshproxy` for Windows users, as detailed [here](https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection).\n\n7. When prompted, enter your username and OTP (if required) to complete the login process.\n\nRemember to adhere to NERSC's login node policies for small testing and debugging, and for more substantial work, utilize CPU or GPU compute nodes as appropriate, as described [here](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/services/jupyter/).\n\nReferences:\n * <https://docs.nersc.gov/connect/mfa/#login-to-nersc-machines>\n * <https://docs.nersc.gov/connect/#connect-to-nersc-computational-systems>\n * <https://docs.nersc.gov/connect/#connecting-with-ssh>",
    "10": "If you need to install a library that is not currently available on Perlmutter, you can follow these steps:\n\n1. **Check for community support**: First, verify if the library has a community-maintained package or a Spack recipe. Spack is commonly used for managing software dependencies at NERSC. You can find Spack packages for various libraries at <https://spack.readthedocs.io/en/latest/index.html>.\n\n2. **Create a Spack package**: If there's no existing Spack package, consider creating one yourself. Follow the Spack documentation (<https://spack.readthedocs.io/en/latest/tutorial/new_package.html>) to create a new Spack package for the library. This typically involves writing a `package.py` file that describes the package and its dependencies.\n\n3. **Install from source**: If you can't find a package, you'll need to compile the library from source. For Cray systems like Perlmutter, ensure you use the appropriate compiler wrappers (`cpe` modules) for the correct version. Here's an example:\n   ```shell\n   module load cpe/<the-non-default-version>\n   export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n   ```\n\n4. **Configure library build**: Depending on the library, you may need to configure the build with the correct options for your system. This might involve setting up environment variables for dependencies like HDF5 or NetCDF, as shown in the WRF build script example:\n   ```shell\n   export HDF5=$HDF5_DIR\n   export NETCDF=$NETCDF_DIR\n   ```\n\n5. **Build and install**: Once configured, build the library using the appropriate commands for your library, and then install it:\n   ```shell\n   ./configure\n   make\n   make install\n   ```\n\n6. **Update module files**: After successful installation, you might need to update your system's module files to include the newly installed library. This is typically done by creating a custom module file and adding it to the system's module path.\n\n7. **Load the new module**: Finally, use the `module load` command to load your custom module:\n   ```shell\n   module load your_library_name\n   ```\n\nRemember that installing from source can be complex, and you may need to adapt the process based on the library's documentation and requirements. If you encounter difficulties, consult the library's documentation, the NERSC User Support, or the library's community forums for help.\n\nReferences:\n * <https://docs.nersc.gov/applications/wrf/wps/#example-build-script-for-the-jasper-external-library>",
    "11": "To view your active and previous job statuses at NERSC, you can use the `squeue` command. Here's how:\n\n1. To check the status of all currently running jobs for the current user:\n```bash\nsqueue --me -t RUNNING\n```\nThis will show you the jobs that are running, along with information like the job ID, partition, name, user, state, and other details.\n\n2. To view all pending jobs (not yet started) for the current user:\n```bash\nsqueue --me -t PENDING\n```\nThis will give you a list of jobs that are awaiting execution.\n\n3. To view all jobs for a particular account or project, use:\n```bash\nsqueue -A <nersc_project>\n```\nReplace `<nersc_project>` with your project name.\n\n4. To view your past job history, including completed jobs, you can use:\n```bash\nsqueue --user=<your_username> --all-users\n```\nReplace `<your_username>` with your actual username.\n\nIf you want to access detailed information about a specific job, you can use the job ID with the `-j` option:\n```bash\nsqueue -j <job_id>\n```\nRemember to replace `<job_id>` with the ID of the job you're interested in.\n\nFor a more comprehensive view, consider using the Iris interface (<https://docs.nersc.gov/iris/iris-for-users/#jobs>) which provides a user-friendly dashboard to monitor job details, transfers, and metrics.\n\nReferences:\n * <https://docs.nersc.gov/jobs/monitoring/#squeue>\n * <https://docs.nersc.gov/iris/iris-for-users/#jobs>\n * <https://docs.nersc.gov/iris/iris-for-users/#jobs-a-namepjobsa>",
    "12": "There could be several reasons why your job is being killed before completion, even if you've verified that there are no apparent errors. Here are some possibilities:\n\n1. **Job Priority and Queue Wait Time**: As mentioned in the documentation, the queue wait time can be affected by the walltime requested, with longer jobs taking longer to start due to scheduling constraints. If your job requires a significant amount of time, it might be competing with other high-priority jobs, causing it to be preempted.\n\n2. **Resource Limits**: NERSC systems may have limits on resources like CPU time, memory, or disk I/O. If your job exceeds these limits, it could be terminated to prevent impacting other users. Check your resource allocation and ensure they are within the allowed bounds.\n\n3. **Queue Policies**: Some queues, like `debug` or `preempt`, have policies that allow for preemption if necessary. Make sure you are using the appropriate queue for your job's requirements.\n\n4. **Signal-based Termination**: Your job might be using a feature like checkpointing and restart, as seen in the examples provided. If the job is configured to checkpoint before the walltime limit, it might receive a signal (like USR1) to prepare for termination, which could result in it finishing its current checkpoint and exiting gracefully.\n\n5. **Error Detection**: Although you didn't find errors, there might be subtle issues that Slurm's monitoring system detects, leading to termination. You can review your job's accounting information with `sacct` to see if there are any unexpected events.\n\n6. **Software Issues**: Sometimes, bugs or misconfigurations in your code or dependencies can cause unexpected behavior. Double-check your code and dependencies to ensure everything is working as intended.\n\n7. **System Maintenance**: NERSC occasionally performs maintenance that could affect running jobs. If there's an underlying system issue, your job might be stopped temporarily.\n\nTo troubleshoot further, try the following steps:\n\n1. Review the `sacct` output for your job to see if there are any error codes, timeouts, or resource usage details.\n2. Check your `SBATCH` flags, especially the `--time` and `--signal` options, to ensure they align with your job's requirements.\n3. If using checkpointing, verify that your job is configured correctly to handle preemption and that it's not causing excessive resource consumption.\n\nIf you're still unsure, contact NERSC support with your specific job ID and details, as they can provide a more tailored analysis based on your job logs and system status.\n\nReferences:\n * <https://docs.nersc.gov/development/checkpoint-restart/dmtcp/#perlmutter>\n * <https://docs.nersc.gov/jobs/troubleshooting/#troubleshooting-jobs>\n * <https://docs.nersc.gov/development/checkpoint-restart/mana/examples/pm-cpu-mana-preempt-vt.sh>\n * <https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu>",
    "13": "To kill all of your jobs at NERSC, you can use the `scancel` command. Here's how:\n\n1. First, ensure you're logged in to the NERSC system with the correct user account.\n\n2. To cancel all jobs owned by your user account, run the following command:\n```bash\nscancel -u $USER\n```\nReplace `$USER` with your actual username.\n\n3. Be cautious when performing this action, as canceling a large number of jobs at once may cause a degradation of service. It's recommended to avoid canceling jobs in bulk and instead cancel them individually if necessary.\n\nRemember to use this command carefully, as it will stop all active jobs associated with your user account. If you need to cancel specific jobs with specific IDs, you can list them separated by spaces:\n```bash\nscancel JobID1 JobID2 JobID3 ...\n```\nKeep in mind that if your jobs are running on multiple nodes, you might need to wait for them to finish gracefully or use the `-f` force option if they are stuck in a state where they cannot be canceled normally.\n\nIf you encounter any issues or need further assistance, feel free to contact NERSC support at `<https://help.nersc.gov>` for help.\n\nReferences:\n * <https://docs.nersc.gov/>\n * <https://www.nersc.gov/users/getting-help/online-help-desk/>",
    "14": "To run a job on the CPU at NERSC, follow these steps:\n\n1. **Choose the appropriate queue**: Select the `cpu` queue when submitting your job, indicating that it requires CPU resources. For instance, use `SBATCH -q regular` or `SBATCH -q debug` depending on the desired priority and resources.\n\n2. **Set the number of nodes and cores**: Specify the number of nodes your job will use with `-N` and the number of CPU cores per node with `-c`. For example, `-N 2 -c 128` allocates two nodes with 128 cores each, allowing for parallel processing.\n\n3. **Load necessary modules**: Ensure you load the required software modules for your application. For example, if you're working with VASP, use `module load vasp/your-version-cpu`.\n\n4. **Write a job script**: Create a `.slurm` script with the `srun` command to execute your program, like `srun -n <num_tasks> -c <threads_per_task> --cpu-bind=cores <your_program>`. Adjust the `num_tasks` and `threads_per_task` according to your needs.\n\n5. **Submit the script**: Use the `sbatch` command to submit your job script to the queue, e.g., `sbatch run.slurm`.\n\nHere's a sample script snippet for running VASP on Perlmutter CPU nodes:\n```bash\n#!/bin/bash\n#SBATCH -N 2\n#SBATCH -C cpu\n#SBATCH -q regular\n#SBATCH -t 01:00:00\n#SBATCH -J vasp_job\n#SBATCH -o %x-%j.out\n#SBATCH -e %x-%j.err\n\n# Load VASP module\nmodule load vasp/your_version-cpu\n\n# Example with VASP\nsrun -n 256 -c 2 --cpu-bind=cores vasp_std\n```\nRemember to adjust the parameters according to your specific requirements and the available queues and resource limits. For more detailed examples, refer to the provided source URLs:\n\n\n\nReferences:\n * <https://docs.nersc.gov/applications/vasp/examples/long-time.sh>\n * <https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu>\n * <https://docs.nersc.gov/applications/vasp/examples/perlmutter-cpu-v5.sh>\n * <https://docs.nersc.gov/applications/vasp/#sample-job-scripts>\n * <https://docs.nersc.gov/development/languages/python/parallel-python/#launching-programs>\n * <https://docs.nersc.gov/jobs/workflow/taskfarmer/#step-3-create-a-batch-script-submittaskfarmersl>",
    "15": "You do not need a dedicated Virtual Private Network (VPN) to access Perlmutter, the NERSC supercomputer, if you are connecting remotely. You can simply use SSH (Secure Shell) to log in, as long as you have an active NERSC account and have set up your SSH configuration correctly. If you have generated an SSH key with `sshproxy`, you can use that to authenticate without needing a password each time. Just follow the instructions for connecting with SSH provided in the documentation:\n\n```\nssh <username>@perlmutter.nersc.gov   # or 'ssh <username>@saul.nersc.gov'\n```\n\nHowever, if you're using a collaboration account, you might need to authenticate with a password and OTP as described in the source URLs. Remember to keep your credentials secure and follow NERSC's best practices for remote access.\n\nReferences:\n * <https://docs.nersc.gov/connect/#connecting-with-ssh>",
    "16": "If your project runs out of allocated time, it will be restricted to certain queues, typically the 'overrun' and 'xfer' queue. Access to the regular or debug queues will be limited. Users associated with a restricted project can still log in but won't be able to submit batch jobs or run parallel jobs, except on the 'overrun' partition. If you have a positive balance in another project or user quota, you'll not face restrictions. To run jobs, you should consider using the overrun queue and implementing checkpoint/restart techniques like VASP examples to handle potential preemptions due to high priority workloads. Remember to adhere to the queue policies and time limits when submitting jobs during such a restriction.\n\nReferences:\n * <https://docs.nersc.gov/policies/resource-usage/#running-out-of-allocation>",
    "17": "To write a script to submit a job to Perlmutter at NERSC, you'll need to follow these steps using the Slurm workload manager. Here's a simple example using a Bash script with the necessary components:\n\n1. **Create a sample job script (job.slurm)**:\n```bash\n#!/bin/bash\n#SBATCH --job-name=<your_job_name>       # Unique identifier for the job\n#SBATCH --output=<your_output_file>.out   # Output file name (stdout)\n#SBATCH --error=<your_output_file>.err   # Error file name (stderr)\n#SBATCH --partition=<queue_name>         # Choose the appropriate partition (e.g., regular, debug, etc.)\n#SBATCH --ntasks=<number_of_tasks>        # Number of tasks (CPUs or GPUs)\n#SBATCH --cpus-per-task=<cpus_per_task>   # If using CPUs, number of cores per task\n#SBATCH --gres=gpu:<gpu_count>           # If using GPUs, specify the number (e.g., 1 for one GPU)\n\n# Application-specific settings\nmodule load <your_module>              # Load required software modules\nyour_command arguments...            # Replace with your VASP, MPI, or any application command\n\n# Example: Running VASP\nmodule load vasp\nmpirun -np <number_of_tasks> vasp_std\n```\nReplace `<your_job_name>`, `<your_output_file>`, `<queue_name>`, `<number_of_tasks>`, `<cpus_per_task>`, `<gpu_count>`, `<your_module>`, and `your_command` with your specific job details.\n\n2. **Set execution permissions**:\n```bash\nchmod +x job.slurm\n```\n3. **Submit the job**:\n```bash\nsbatch job.slurm\n```\nAlternatively, if you prefer to use the SF API for more advanced submission options, you can use Python with the `requests` library, as shown in the source URL:\n\n```python\nimport requests\n\nsystem = \"perlmutter\"\nsubmit_script_path = \"/path/to/your/job.slurm\"  # Replace with the actual script path\ndata = {\"job\": open(submit_script_path, \"rb\"), \"isPath\": True}\n\nresponse = requests.post(\"https://api.nersc.gov/api/v1.2/compute/jobs/\" + system, files=data)\ntask_id = response.json()[\"task_id\"]\n```\nRemember to replace the `system`, `submit_script_path`, and other placeholders with your actual values.\n\nFor more detailed examples, refer to the source URLs provided:\n\n- [Example job scripts](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/examples/)\n- [Job script generator](https://iris.nersc.gov/jobscript)\n- [Batch job submission](https://docs.nersc.gov/services/sfapi/examples/#batch-job-submission)\n\nKeep in mind that you may need to adjust the script according to your application requirements and NERSC policies. Also, ensure you have the necessary permissions to submit jobs.\n\nReferences:\n * <https://docs.nersc.gov/services/sfapi/examples/#batch-job-submission>",
    "18": "To set up Multi-Factor Authentication (MFA) at NERSC, follow these steps:\n\n1. **Install an Authenticator App**: Download and install a popular authenticator app like Google Authenticator, Authy, or Microsoft Authenticator on your smartphone or another trusted device.\n\n2. **Enable MFA in Iris**: Log in to your Iris account at <https://iris.nersc.gov>. If you haven't already, click the \"MFA not working?\" link on the login page.\n\n3. **Request a One-Time Password (OTP)**: Enter your username and password, then click on the option to create a MFA token. NERSC will send a one-time password (OTP) to your registered email address.\n\n4. **Enter OTP**: Use the received OTP to proceed with the MFA setup. This may involve verifying the OTP in your chosen authenticator app.\n\n5. **Generate a NERSC Token**: In Iris, generate a new MFA token. This will create a unique code that will be displayed in the authenticator app.\n\n6. **Install the Token**: Add the NERSC-generated token to your authenticator app by scanning the QR code provided.\n\n7. **Verify and Save**: After installing the token, verify that it's working correctly. If prompted, enter the token generated by the app when prompted for a second factor during login.\n\n8. **Delete Old Tokens (if needed)**: If you've lost your previous tokens, you may need to delete them from your authenticator app and start fresh with the new one.\n\nFor detailed instructions and screenshots, refer to the NERSC documentation on [setting up MFA](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#configuring-and-using-an-mfa-token).\n\nReferences:\n * <https://docs.nersc.gov/accounts/passwords/#multi-factor-authentication>",
    "19": "There could be several reasons for a login failure at NERSC. Here are some common issues and their solutions:\n\n1. Incorrect password: If you've entered your password incorrectly ten times, your account may be locked for 5 minutes. Simply wait and try again after that time.\n\n2. MFA (Multi-Factor Authentication): If MFA is enabled, make sure you're entering the correct One-Time Password (OTP) or following the instructions to unlock your account if it's locked due to too many failed attempts.\n\n3. Expired credentials: Check if your password has expired. If so, update it through the Iris portal.\n\n4. Login failures: Visit [Iris](https://iris.nersc.gov) and clear any existing login failures under your profile.\n\n5. Key-based authentication issues: If using ssh keys, ensure they are valid and not expired. You might need to manage your ssh-agent keys.\n\n6. SSH connection issues: Try connecting to the NoMachine server directly with `ssh <nersc_username>@nxcloud01.nersc.gov` to check if there's a network issue.\n\n7. Local firewall: Confirm that your local firewall isn't blocking the connection.\n\nIf none of these resolve the issue, open a ticket with the NERSC Help Portal at <https://help.nersc.gov> for further assistance.\n\nReferences:\n * <https://docs.nersc.gov/connect/nx/#troubleshooting-nx-connection-failures>\n * <https://docs.nersc.gov/connect/mfa/#q-i-have-enabled-mfa-my-logins-fail-repeatedly-what-should-i-do>\n * <https://docs.nersc.gov/accounts/passwords/#login-failures>",
    "20": "When choosing a file system at NERSC to store your data, consider the following factors:\n\n1. **Purpose**: Different file systems serve specific purposes. For production I/O and high-performance access, use [Perlmutter Scratch](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/perlmutter-scratch/). This file system is optimized for fast access and is best suited for data that is actively used during a job.\n\n2. **Data Persistence**: If your data requires long-term storage and less frequent access, HPSS (Hierarchical Persistent Storage System) is more appropriate, but it does not offer snapshots or automatic backup.\n\n3. **Durability**: NERSC's file systems are not designed as disaster-proof storage. Ensure you have redundant copies of your data elsewhere.\n\n4. **Intended Use**: Do not use your home directory for production data. Instead, use the Common or Community file systems for project data, depending on whether you need snapshots and backup.\n\n5. **Scalability**: Large files and high concurrency can benefit from avoiding \"file-per-process\" models and preferring contiguous access.\n\n6. **Temporary Storage**: If you need temporary scratch space, use the per-node XFS file system for Shifter users or the local temporary file system on compute nodes.\n\nIn summary, for active, high-performance data, use Perlmutter Scratch. For long-term, less-frequently accessed data, consider HPSS. Always follow the guidelines for intended use and data management to ensure optimal performance and data security.\n\nReferences:\n * <https://docs.nersc.gov/filesystems/archive/#best-practices>"
}
