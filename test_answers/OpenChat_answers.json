{
    "1": "To join the NERSC Users Group (NUG) Slack workspace, follow these steps:\n\n1. Visit the NUG Slack workspace invitation page at <https://www.nersc.gov/users/NUG/nersc-users-slack/>.\n2. Click on the \"Join NERSC Users Slack Workspace\" button.\n3. You will be redirected to the Slack registration page. If you don't have a Slack account, create one by clicking \"Sign Up\" and following the instructions. If you already have a Slack account, log in using your credentials.\n4. After logging in or signing up, you will be prompted to confirm your email address. Check your email inbox for a confirmation email from Slack, and click the confirmation link to verify your email address.\n5. Once your email address is verified, you will be redirected back to the NERSC Users Slack workspace invitation page. Click the \"Authorize\" button to join the workspace.\n\nAfter completing these steps, you will have successfully joined the NERSC Users Group Slack workspace.\n\nReferences:\n * <https://www.nersc.gov/users/NUG/nersc-users-slack/>",
    "2": "Users can open support tickets and make resource requests by accessing the [NERSC Help Portal](https://help.nersc.gov). This is the preferred method for contacting NERSC for support.\n\nReferences:\n * <https://docs.nersc.gov/getting-started/#how-to-file-a-good-ticket>",
    "3": "If you want to run a short job on a GPU at NERSC, you should use the \"debug\" QOS. This QOS allows you to request a single GPU node for up to 20 hours of wall time. You can request a GPU node using the following `salloc` command:\n\n```\nsalloc --nodes 1 --qos debug --time 20:00 --constraint gpu --account=mxxxx\n```\n\nReplace `mxxxx` with the account you want to charge hours from. Once you have an allocation, connect to the node using the node name starting with `nid` as the hostname.\n\nReferences:\n * <https://docs.nersc.gov/jobs/policy/#perlmutter-gpu>\n * <https://docs.nersc.gov/jobs/interactive/#perlmutter-debug-qos>",
    "4": "In a Slurm job, mandatory flags are those that are essential for the job to be submitted and executed. Some of the common mandatory flags include:\n\n1. `-p`: Partition or queue to which the job is submitted.\n2. `-n` or `--ntasks`: Number of tasks or processes to be run.\n3. `-N`: Name of the job.\n4. `-t` or `--time`: Maximum runtime of the job in hours or minutes.\n5. `-J`: Job name for the array job.\n6. `-b` or `--exclusive`: Requests an exclusive partition.\n7. `-c` or `--cpus-per-task`: Number of CPUs per task.\n8. `-o` or `--output`: File to which the standard output is redirected.\n9. `-e` or `--error`: File to which the standard error is redirected.\n10. `-w` or `--workdir`: Working directory for the job.\n\nOptional flags are those that can be used to customize the job's behavior or provide additional information. Some common optional flags include:\n\n1. `-A`: Allocation to be used for the job.\n2. `-M` or `--mail-type`: Type of email notification to be sent.\n3. `-m` or `--mail-type=begin|end|fail|requeue|all`: Specifies the type of email notification to be sent.\n4. `--mail-user`: Email address to which the notifications are sent.\n5. `--job-name`: Overrides the default job name.\n6. `--partition`: Overrides the default partition.\n7. `--time`: Overrides the default maximum runtime.\n8. `--nodes`: Overrides the default number of nodes.\n9. `--ntasks-per-node`: Overrides the default number of tasks per node.\n10. `--cpus-per-task`: Overrides the default number of CPUs per task.\n11. `--exclusive`: Overrides the default exclusive partition.\n12. `--output`: Overrides the default output file.\n13. `--error`: Overrides the default error file.\n14. `--workdir`: Overrides the default working directory.\n\nPlease note that the availability and behavior of these flags may vary depending on the Slurm version and the specific installation at your organization.\n\nReferences:\n * <https://docs.nersc.gov/>\n * <https://www.nersc.gov/users/getting-help/online-help-desk/>",
    "5": "The High Performance Storage System (HPSS) is a modern, flexible, performance-oriented mass storage system used at NERSC for archival storage. It has been in use since 1998 and is intended for long-term storage of data that is not frequently accessed. HPSS is a Hierarchical Storage Management (HSM) software developed by a collaboration of DOE labs, of which NERSC is a participant, and IBM. The HPSS system is a tape system that uses HSM software to ingest data onto a high-performance disk cache and automatically migrate it to a very large enterprise tape subsystem for long-term retention. The disk cache in HPSS is designed to retain many days worth of new data, and the tape subsystem is designed to provide the most cost-effective long-term scalable data storage available.\n\nReferences:\n * <https://docs.nersc.gov/filesystems/#archivehttpsdocsnerscgovglobalcfscdirsnstaffchatbotproductioncodedatanerscdocdocsfilesystemsarchive-hpss>\n * <https://docs.nersc.gov/filesystems/archive/#introduction>",
    "6": "To find the location of the `gcc` compiler on the NERSC systems, you can use the `which` command in the terminal. For example, you can type the following command:\n\n```shell\nwhich gcc\n```\n\nThis command will display the full path to the `gcc` compiler on your system. Note that the output may vary depending on the version of `gcc` installed on your system.\n\nReferences:\n * <https://docs.nersc.gov/development/build-tools/spack/#unloading-spack-packages-from-your-environment>",
    "7": "To use sshproxy, follow these steps:\n\n1. First, download the sshproxy client from NERSC. You can do this using the command `scp myusername@dtn01.nersc.gov:/global/cfs/cdirs/mfa/NERSC-MFA/sshproxy.sh .` Replace `myusername` with your NERSC login ID.\n\n2. Make the sshproxy script executable by running the command `chmod +x sshproxy.sh`.\n\n3. Run the sshproxy script with the `-u` option to specify your NERSC username, and the `-o` option to specify the output filename for the private key. For example, `./sshproxy.sh -u myusername -o mynersc`.\n\n4. Enter your NERSC password followed by your OTP when prompted.\n\n5. Once the key is obtained, you can use it to ssh to NERSC systems without further authentication until the key expires.\n\nRemember to replace `myusername` with your actual NERSC username and `mynersc` with your desired output filename for the private key.\n\nReferences:\n * <https://docs.nersc.gov/connect/mfa/#sshproxy>\n * <https://docs.nersc.gov/connect/mfa/#using-sshproxy>",
    "8": "To unsubscribe from the NERSC users email list, you cannot do it yourself as the list is managed by NERSC and users cannot unsubscribe from it due to the important nature of the information. However, you can contact the NERSC support staff for assistance or ask a project PI or one of their proxies to help you with this issue.\n\nReferences:\n * <https://docs.nersc.gov/getting-started/#email>",
    "9": "To connect to Perlmutter, you can use SSH (Secure Shell) to log into the system. Here are the steps to follow:\n\n1. Open a terminal on your local machine.\n2. Run the following command to connect to Perlmutter using SSH:\n\n   ```\n   ssh <username>@perlmutter.nersc.gov\n   ```\n\n   or\n\n   ```\n   ssh <username>@saul.nersc.gov\n   ```\n\nIf you have configured sshproxy, you can use the following command:\n\n```\nssh -i ~/.ssh/nersc <username>@perlmutter.nersc.gov   # or 'ssh -i ~/.ssh/nersc <username>@saul.nersc.gov'\n```\n\nThis assumes your identity file is in `~/.ssh/nersc`. The sshproxy route will be convenient if you have multiple SSH connections without having to authenticate every time.\n\nIf you are using Windows, you can use the NoMachine client to connect to Perlmutter. Follow these steps:\n\n1. Download and install the NoMachine client from the NERSC website.\n2. Open the NoMachine client and click the yellow Connect button at the top right.\n3. Enter your NERSC username and password.\n4. Verify the key and create a new desktop.\n5. You should land at the NERSC homescreen.\n6. Click the green buttons on the left-hand menu to connect to Perlmutter.\n\nRemember to replace `<username>` with your actual NERSC username.\n\nReferences:\n * <https://docs.nersc.gov/connect/#connect-to-nersc-computational-systems>\n * <https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection>\n * <https://docs.nersc.gov/connect/#connecting-with-ssh>",
    "10": "To install and use a library that currently does not exist on Perlmutter, you can follow these steps:\n\n1. **Check if the library is available in the Spack ecosystem**: Spack is a package manager that simplifies the installation of software on HPC systems. It is widely used at NERSC and might have the library you need. You can search for the library in the Spack ecosystem by visiting <https://spack.io/search>.\n\n2. **Install the library using Spack**: If the library is available in the Spack ecosystem, you can install it using the `spack` command. For example, if you want to install the `zlib` library, you can run the following command:\n```\nspack install zlib\n```\n\n3. **Load the library**: After installing the library, you need to load it in your environment. You can do this using the `module` command. For example, to load the `zlib` library, you can run:\n```\nmodule load zlib\n```\n\n4. **Use the library in your code**: Once the library is loaded, you can use it in your code as you would normally do.\n\nIf the library is not available in the Spack ecosystem, you may need to build it from source. In this case, you can follow the instructions provided by the library's developers or maintainers.\n\nRemember to always check the NERSC documentation and guidelines for installing and using software on their systems, as there might be specific requirements or restrictions.\n\nReferences:\n * <https://docs.nersc.gov/development/languages/python/using-python-perlmutter/#cunumeric>",
    "11": "To view your active and previous job statuses at NERSC, you can use the `squeue` command for active jobs and the `sacct` command for previous jobs.\n\nFor active jobs, you can use the following command:\n```\nsqueue --me -t RUNNING\n```\nThis command will display all running jobs for the current user.\n\nFor previous jobs, you can use the `sacct` command with the `--starttime` and `--endtime` flags to specify the date range you want to view. For example:\n```\nsacct --starttime 2022-01-01 --endtime 2022-01-31\n```\nThis command will display all jobs that were run between January 1, 2022, and January 31, 2022.\n\nYou can also use the `sacct` command with the `--jobid` flag to view details about a specific job. For example:\n```\nsacct --jobid=12345\n```\nThis command will display details about the job with ID 12345.\n\nPlease note that these commands and options may vary depending on the NERSC system you are using. Always refer to the official NERSC documentation for the most accurate and up-to-date information.\n\nReferences:\n * <https://docs.nersc.gov/jobs/monitoring/#squeue>",
    "12": "There could be several reasons why your job is being killed before it finishes. Some possible causes include:\n\n1. **Resource limits**: Your job may be exceeding the resource limits set by the NERSC supercomputing center. These limits can include the maximum wall time, the maximum number of nodes, or the maximum amount of memory usage. You can check the resource limits for your job by examining the job submission script and the job scheduling system.\n\n2. **Time limit**: Your job may be hitting the time limit set for the job. This can happen if the job is running longer than the maximum allowed wall time. To resolve this issue, you can try reducing the maximum wall time for your job or optimizing your code to run more efficiently.\n\n3. **System policies**: The NERSC supercomputing center may have specific policies in place that can cause jobs to be killed. These policies can include preemption, where jobs are terminated to free up resources for other jobs, or resource reservation policies that prioritize certain types of jobs. You can check the NERSC documentation for more information on their policies.\n\n4. **Bug in the code**: There may be a bug in your code that is causing it to run indefinitely or consume excessive resources. To identify and fix the bug, you can try debugging your code or using profiling tools to analyze its performance.\n\nTo further investigate the issue, you can check the job accounting information using the `sacct` command, which can provide details on the job's status, wall time, and resource usage. Additionally, you can examine the job submission script and any error messages to identify potential causes for the job being killed.\n\n\n\nReferences:\n * <https://docs.nersc.gov/jobs/troubleshooting/#troubleshooting-jobs>\n * <https://docs.nersc.gov/jobs/monitoring/#job-accounting>",
    "13": "To kill all of your jobs at NERSC, you can use the `scancel` command. This command allows you to cancel a specific job by its Job ID or cancel all jobs owned by a user.\n\nTo cancel all jobs owned by a user, use the following command:\n```\nscancel -u $USER\n```\n\nPlease note that canceling too many jobs at once can cause a degradation of service. It is recommended to cancel jobs by subset instead of all at once.\n\n\n\nReferences:\n * <https://docs.nersc.gov/jobs/monitoring/#updating-jobs>",
    "14": "To run a job on a CPU at NERSC, you can use the Slurm Workload Manager, which is the job scheduling system at NERSC. Here's a concise guide on how to do it:\n\n1. **Prepare your environment**: Before submitting your job, you may need to prepare your environment by loading the necessary modules and setting up any required environment variables. For example, you can use the `module load` command to load the required modules.\n\n2. **Create a Slurm job script**: Write a Slurm job script that describes your job, including the number of nodes, the time limit, the partition (e.g., `regular`, `debug`, `debug_preempt`, etc.), and the job name. The script should also specify the commands to run your job.\n\n3. **Submit the job**: Submit your job script to the Slurm scheduler using the `sbatch` command. For example:\n\n```shell\nsbatch my_job_script.slurm\n```\n\nHere's a sample Slurm job script for running a job on a CPU:\n\n```slurm\n#!/bin/bash\n#SBATCH -N 1\n#SBATCH -C cpu\n#SBATCH -J my_job\n#SBATCH -o %x-%j.out\n#SBATCH -e %x-%j.err\n#SBATCH --qos=debug\n#SBATCH --time=00:30:00\n#SBATCH --mem=4G\n\nmodule load vasp\n\nsrun -n 128 -c 2 --cpu-bind=cores vasp_std\n```\n\nIn this example, the job requests 1 node with CPU access, a job name of \"my_job\", and a time limit of 30 minutes. The `srun` command is used to run the `vasp_std` program with 128 tasks and 2 logical cores per task.\n\nRemember to replace the example values in the script with the appropriate values for your specific job.\n\n\n\nReferences:\n * <https://docs.nersc.gov/jobs/workflow/taskfarmer/#step-3-create-a-batch-script-submittaskfarmersl>\n * <https://docs.nersc.gov/development/languages/python/parallel-python/#launching-programs>\n * <https://docs.nersc.gov/applications/vasp/examples/perlmutter-cpu-v5.sh>\n * <https://docs.nersc.gov/jobs/examples/prepare-env/prepare-env.sh>",
    "15": "You do not need a VPN to access Perlmutter if you are offsite. You can simply use the SSH (Secure Shell) protocol to connect to Perlmutter. The most common way to do this is by using the `ssh` command in a terminal or command prompt.\n\nHere is the command to connect to Perlmutter using SSH:\n\n```\nssh <username>@perlmutter.nersc.gov\n```\n\nReplace `<username>` with your NERSC username. If you have generated a temporary SSH key using `sshproxy`, this command will connect you to one of Perlmutter's login nodes. If you have not generated a temporary SSH key, SSH will challenge you for your NERSC password and one-time password (OTP) before connecting you to a login node.\n\nPlease note that you should use the most up-to-date versions of SSH clients available to ensure a secure connection.\n\nReferences:\n * <https://docs.nersc.gov/connect/#connecting-with-ssh>",
    "16": "If your project runs out of time, you will be restricted to the \"overrun\" and \"xfer\" queues. In this case, you will not be able to charge to the restricted project, and your jobs will be stuck indefinitely if you try to run them on default queues. It is recommended to change your default project through Iris or specify a different project with sufficient allocation when submitting a job, or run jobs in the \"overrun\" queue.\n\nReferences:\n * <https://docs.nersc.gov/policies/resource-usage/#running-out-of-allocation>",
    "17": "To write a script to submit a job to Perlmutter, you can follow these steps:\n\n1. Prepare a job script for your specific application. The job script should include the necessary commands to run your application, as well as any required environment modules, resource requests, and other configurations.\n\n2. Save the job script to a file on the NERSC file system, for example, `/global/homes/u/username/script.sub`.\n\n3. Use the SF API to submit the job script to the Slurm scheduler. You can do this by sending a `POST` request to the appropriate API endpoint with the job script file path as a parameter. Here's an example using Python and the `requests` library:\n\n```python\nimport requests\n\nsystem = \"perlmutter\"\nsubmit_script = \"/global/homes/u/username/script.sub\"\n\nr = requests.post(f\"https://api.nersc.gov/api/v1.2/compute/jobs/{system}\",\n                  data = {\"job\": submit_script, \"isPath\": True})\n\nprint(r.json())\n```\n\nThe result output will include the SF API task ID, the status of the `POST` request, and any error text. Note that the `POST` request status does not indicate whether the job submission was accepted by the Slurm scheduler.\n\nFor more information on writing job scripts and using the SF API, refer to the NERSC documentation on [batch job submission](https://docs.nersc.gov/services/sfapi/examples/#batch-job-submission) and [sample job scripts](https://docs.nersc.gov/applications/vasp/examples/perlmutter-cpu-v6.sh).\n\nReferences:\n * <https://docs.nersc.gov/services/sfapi/examples/#batch-job-submission>",
    "18": "To set up Multi-Factor Authentication (MFA) at NERSC, follow these steps:\n\n1. Log in to Iris using your NERSC username and password.\n2. Click the 'MFA not working?' link on the Iris login page.\n3. Enter your username and password, then click OK.\n4. A dialog box will appear, asking if you want to create a MFA token. Click OK.\n5. A new dialog box will show up. If you have entered the correct password, NERSC will send an email containing a single-use One-Time Password (OTP).\n6. Use the OTP to log in to Iris.\n7. Once logged in, create and install a new MFA token by following the instructions in the 'Configuring and Using an MFA Token' section of the NERSC MFA documentation.\n\n\n\nReferences:\n * <https://docs.nersc.gov/connect/mfa/#configuring-and-using-an-mfa-token>\n * <https://docs.nersc.gov/connect/mfa/#if-you-lost-your-tokens>",
    "19": "There could be several reasons why you are unable to log in. Some common issues include:\n\n1. **Incorrect username or password**: Ensure that you are using the correct username and password for your NERSC account. If you have forgotten your password, you can reset it by visiting the NERSC Account Support Office at <https://docs.nersc.gov/accounts/passwords/#resetting-your-password>.\n2. **Login failures**: If you have ten login failures while entering your password, your login privileges will be disabled for 5 minutes. The login failures will automatically clear after this period. No additional actions are necessary.\n3. **MFA issues**: If you have enabled Multi-Factor Authentication (MFA), you may be experiencing login issues. If you enter incorrect One-Time Passwords (OTPs) too many times, the NERSC MFA server locks you out. In this case, you have to wait for 15 minutes before trying again.\n4. **IP address issues**: If you are accessing web services and see IP addresses in the 10.42.0.0/16 range in your access logs, it is likely due to the ingress controller in Spin. To change this behavior, configure your web service to obtain the IP address from the `X-Forwarded-For` HTTP header.\n\nTo troubleshoot your specific issue, you can try the following steps:\n\n1. Log into [Iris](https://iris.nersc.gov) to clear any login failures. Access to NoMachine uses your NERSC user name and password.\n2. Create a new connection file following the instructions provided in the NERSC documentation.\n3. Try to ssh directly to the NoMachine server using the command `ssh <nersc_username>@nxcloud01.nersc.gov`.\n\nIf you continue to experience issues, contact the NERSC Account Support Office by sending an email to [accounts@nersc.gov](mailto:accounts@nersc.gov) or open a trouble ticket from the NERSC Help Portal at <https://help.nersc.gov>.\n\nReferences:\n * <https://docs.nersc.gov/connect/nx/#troubleshooting-nx-connection-failures>\n * <https://docs.nersc.gov/connect/mfa/#q-i-have-enabled-mfa-my-logins-fail-repeatedly-what-should-i-do>\n * <https://docs.nersc.gov/accounts/passwords/#login-failures>\n * <https://docs.nersc.gov/services/jupyter/reference/#jupyterlab-server-logs>\n * <https://docs.nersc.gov/development/languages/python/faq-troubleshooting/#why-does-my-mpi4py-time-out-or-why-is-it-so-slow>\n * <https://docs.nersc.gov/services/spin/faq/#why-are-ip-addresses-in-the-10420016-range-showing-in-my-web-service-access-log>\n * <https://docs.nersc.gov/jobs/monitoring/#how-to-log-into-compute-nodes-running-your-jobs>\n * <https://docs.nersc.gov/iris/iris-for-users/#problems-accessing-iris>",
    "20": "When choosing a file system to store your data at NERSC, it's important to consider the purpose of the file system and the nature of your data. Here are some general guidelines:\n\n1. **Home**: Use your home directory for small amounts of data that you need to access frequently. This directory is backed up and is a good place to store configuration files or other small files that you need to access regularly.\n\n2. **Common**: The Common file system is suitable for larger datasets that are not accessed as frequently as those stored in your home directory. It is not backed up, so use it for data that is not critical to your work.\n\n3. **Community**: The Community file system is also suitable for larger datasets and is not backed up. It is designed for collaborative projects, so use it if you are working with others on a project and need to share data.\n\n4. **Perlmutter Scratch**: This file system is intended for temporary storage of large datasets that are needed during a job or computation. It is not backed up, and data is purged after a certain period, so use it for data that is only needed for a short time.\n\n5. **HPSS**: The High-Performance Storage System (HPSS) is designed for long-term storage of data that is not frequently accessed. It is suitable for archiving data that you need to keep for an extended period but do not need to access regularly.\n\nRemember that NERSC storage systems are not designed for disaster-proof data storage, so always keep at least one copy of your data at another institution or cloud service.\n\nReferences:\n * <https://docs.nersc.gov/performance/io/dvs/#read-your-data-from-the-right-place>\n * <https://docs.nersc.gov/performance/io/#best-practices-for-scientific-io>\n * <https://docs.nersc.gov/filesystems/#summary>"
}
