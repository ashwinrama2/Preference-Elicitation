{
    "1": "To join the NERSC Users Group (NUG) Slack workspace, follow these steps:\n\n1. Visit the NERSC Users Slack workspace registration page at <https://www.nersc.gov/users/NUG/nersc-users-slack/>.\n2. Click on the \"Join Slack\" button.\n3. Sign in with your preferred method (e.g., Google, Apple, or email).\n4. Complete the registration process by providing the required information.\n5. Once registered, you will be redirected to the NERSC Users Slack workspace, where you can access various channels, including the #spin and #jupyter channels for specific discussions related to Spin and Jupyter.\n\nFor more information about the NERSC Users Group and its activities, you can visit the NUG website at <https://www.nersc.gov/users/NUG/>.\n\n\n\nReferences:\n * <https://www.nersc.gov/users/NUG/nersc-users-slack/>\n * <https://www.nersc.gov/users/NUG/>",
    "2": "Users can open support tickets and make resource requests through the NERSC Help Portal, which can be accessed at <https://help.nersc.gov>. This is the preferred method for contacting NERSC for support-related inquiries and resource allocation requests.\n\nHere are the relevant sources used to provide this answer:\n\n1. <https://docs.nersc.gov/getting-started/#how-to-file-a-good-ticket>\n\n2. <https://docs.nersc.gov/services/jupyter/#getting-help-with-jupyter>\n\nReferences:\n * <https://docs.nersc.gov/services/jupyter/#getting-help-with-jupyter>\n * <https://docs.nersc.gov/getting-started/#how-to-file-a-good-ticket>",
    "3": "For a short GPU job at NERSC, you should consider using the \"debug\" QOS. This QOS provides access to CPU and GPU nodes and is suitable for shorter jobs that don't require a long walltime. Here's an example of how to submit a GPU job using the \"debug\" QOS:\n```bash\nsalloc --nodes 1 --qos debug --time 20:00 --constraint gpu --account=mxxxx\n```\nPlease note that the maximum walltime for the \"debug\" QOS is 20 hours. If you need more time, you can use the \"preempt\" QOS, which allows a minimum of 2 hours of walltime, but your job may be subject to preemption after that time.\n\nHere are the most relevant sources used to create this answer:\n\n1. <https://docs.nersc.gov/jobs/policy/#perlmutter-gpu>\n2. <https://docs.nersc.gov/jobs/interactive/#perlmutter-debug-qos>\n\nReferences:\n * <https://docs.nersc.gov/jobs/policy/#perlmutter-gpu>\n * <https://docs.nersc.gov/jobs/interactive/#perlmutter-debug-qos>",
    "4": "For a Slurm job, the mandatory flags are those that are essential for the job to be submitted and executed correctly. Some of the most common mandatory flags include:\n\n1. `-p` or `--partition`: Specifies the partition or queue on which the job should be executed. This flag is essential to ensure that the job is submitted to the appropriate queue.\n2. `-n` or `--ntasks`: Specifies the total number of tasks or processes that the job should consist of. This flag is necessary to allocate the correct number of resources for the job.\n3. `-c` or `--cpus-per-task`: Specifies the number of CPU cores allocated to each task. This flag is important to ensure that the job is executed with the correct level of parallelism.\n4. `-t` or `--time`: Specifies the maximum runtime of the job in hours, minutes, and seconds. This flag is essential to prevent the job from running indefinitely and consuming excessive resources.\n5. `-o` or `--output`: Specifies the file to which the standard output of the job should be redirected. This flag is useful for managing the output of the job and preventing it from being displayed on the terminal.\n6. `-e` or `--error`: Specifies the file to which the standard error of the job should be redirected. This flag is similar to the `-o` flag but is used for managing the error output of the job.\n7. `-J` or `--job-name`: Specifies a name for the job that will be used in the output and error files, as well as in the Slurm job manager. This flag is useful for identifying the job in the Slurm job manager and in the output and error files.\n\nOptional flags, on the other hand, are those that can be used to customize the behavior of the job or to provide additional information to Slurm. Some common optional flags include:\n\n1. `-A` or `--account`: Specifies the project or allocation to which the job should be charged. This flag is useful for tracking resource usage and allocating costs to specific projects.\n2. `-M` or `--mail-type`: Specifies the type of email notifications that should be sent when the job changes its state. This flag is useful for keeping users informed about the progress and status of their jobs.\n3. `-m` or `--mail-user`: Specifies the email address to which the notifications should be sent. This flag is useful for ensuring that the correct user receives the notifications.\n4. `-W` or `--wait`: Specifies the maximum amount of time that Slurm should wait for the job to start before considering it to be stuck. This flag is useful for preventing the job from being stuck in the queue indefinitely.\n5. `-d` or `--dependency`: Specifies the dependency relationship between the current job and other jobs. This flag is useful for managing the order in which jobs are executed and for ensuring that all necessary dependencies are satisfied.\n6. `-D` or `--exclusive`: Specifies that the job should be executed exclusively on the allocated resources, preventing other jobs from using them. This flag is useful for ensuring that the job has exclusive access to the resources it requires.\n7. `-G` or `--group-by`: Specifies the attribute by which jobs should be grouped for scheduling purposes. This flag is useful for managing the allocation of resources and for ensuring that jobs with similar characteristics are executed together.\n\nThese are just a few examples of mandatory and optional flags for a Slurm job. The specific flags available and their usage may vary depending on the Slurm version and the system configuration.\n\nReferences:\n * <https://docs.nersc.gov/>\n * <https://www.nersc.gov/users/getting-help/online-help-desk/>",
    "5": "The High Performance Storage System (HPSS) is a modern, flexible, performance-oriented mass storage system designed for long-term storage of data that is not frequently accessed. It has been used at NERSC for archival storage since 1998. HPSS is intended to provide the most cost-effective long-term scalable data storage available, with a high capacity tape archive system.\n\nHere are the most relevant sources for this information:\n\n1. <https://docs.nersc.gov/filesystems/archive/#introduction>\n2. <https://docs.nersc.gov/filesystems/#archivehttpsdocsnerscgovglobalcfscdirsnstaffchatbotproductioncodedatanerscdocdocsfilesystemsarchive-hpss>\n\nTo optimize your code for CPU usage at NERSC, it's crucial to focus on vectorization and parallelization. Vectorization allows your code to process multiple data points with a single instruction, effectively reducing the time your code takes to run through large datasets. Parallelization, on the other hand, involves dividing your code into multiple tasks that can be processed simultaneously, maximizing the use of available CPU resources. Combining these two strategies can lead to significant improvements in your code's performance on NERSC systems.\n\nHere are the most relevant sources for this information:\n\n1. <https://docs.nersc.gov/performance/vectorization>\n2. <https://docs.nersc.gov/performance/parallelism>\n\nLocal storage at NERSC provides high I/O performance but often doesn't preserve data across different jobs, so it's meant to be used as scratch space, and data produced must be staged out at the end of the computation. The following file systems are available for local storage:\n\n1. Temporary per-node Shifter file system: <https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/development/containers/shifter/how-to-use/#temporary-xfs-files-for-optimizing-io>\n2. Local temporary file system: <https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/performance/io/dev-shm/>\n\nHere are the most relevant sources for this information:\n\n1. <https://docs.nersc.gov/filesystems/#local-storage>\n\nFor archival storage at NERSC, the High Performance Storage System (HPSS) is provided. It offers long-term archival storage to users at the facility, with a focus on data stewardship and preservation, as well as some data sharing needs.\n\nHere are the most relevant sources for this information:\n\n1. <https://docs.nersc.gov/policies/data-policy/policy/#archival-storage-hpss>\n\nReferences:\n * <https://docs.nersc.gov/performance/parallelism>\n * <https://docs.nersc.gov/filesystems/#archivehttpsdocsnerscgovglobalcfscdirsnstaffchatbotproductioncodedatanerscdocdocsfilesystemsarchive-hpss>\n * <https://docs.nersc.gov/filesystems/#local-storage>\n * <https://docs.nersc.gov/performance/vectorization>\n * <https://docs.nersc.gov/filesystems/archive/#introduction>\n * <https://docs.nersc.gov/policies/data-policy/policy/#archival-storage-hpss>",
    "6": "To find the location of GCC (gcc) in your file path, you can use the `which` command in the terminal. Here's how you can do it:\n\n1. Open a terminal window.\n2. Type the following command and press Enter:\n```bash\nwhich gcc\n```\nThis command will display the path to the GCC binary on your system. The output will look something like this:\n```\n/usr/bin/gcc\n```\nIn this example, the GCC binary is located at `/usr/bin/gcc`. \n\nPlease note that the actual path may vary depending on your system's configuration and the version of GCC installed.\n\nReferences:\n * <https://docs.nersc.gov/development/compilers/base/#gnu>",
    "7": "To use sshproxy at NERSC, follow these steps:\n\n1. **Install the client**: Download the bash client `sshproxy.sh` via scp:\n\n   ```\n   scp myusername@dtn01.nersc.gov:/global/cfs/cdirs/mfa/NERSC-MFA/sshproxy.sh .\n   ```\n\n   Replace `myusername` with your NERSC login ID.\n\n2. **Run sshproxy**: Execute the downloaded `sshproxy.sh` script with the appropriate command-line options. For example, to specify your NERSC username and output filename, use the following command:\n\n   ```\n   ./sshproxy.sh -u myusername -o mynersc\n   ```\n\n   Replace `myusername` with your NERSC login ID and `mynersc` with your desired output filename.\n\n3. **Use the generated key**: Once the ssh key is generated, you can use it to ssh to NERSC systems without further authentication until the key expires (default 24 hours).\n\nFor Windows users, download the Windows executable `sshproxy.exe` from the following location:\n\n```\n   /global/cfs/cdirs/mfa/NERSC-MFA/sshproxy.exe\n```\n\nTo configure a NERSC connection using sshproxy with NoMachine, follow the instructions provided in the NERSC documentation.\n\n\n\nReferences:\n * <https://docs.nersc.gov/connect/mfa/#sshproxy>\n * <https://docs.nersc.gov/connect/mfa/#installing-the-client>\n * <https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection>",
    "8": "As a NERSC user, you are automatically subscribed to the NERSC users email list. Unfortunately, you cannot unsubscribe from this list due to the important nature of the information it provides. The NERSC users email list keeps you informed about important updates, announcements, and system status details related to the NERSC supercomputing center.\n\nTo stay updated with the latest news and announcements from NERSC, it's recommended to keep your subscription active and check the email list regularly.\n\nReferences:\n * <https://docs.nersc.gov/>\n * <https://www.nersc.gov/users/getting-help/online-help-desk/>",
    "9": "To connect to Perlmutter at the NERSC supercomputing center, you can follow these steps:\n\n1. **Direct Login:** Use the `ssh` command to log into Perlmutter with your username. The command is:\n\n   ```\n   ssh <user>@perlmutter.nersc.gov\n   ```\n\n   or\n\n   ```\n   ssh <user>@saul.nersc.gov\n   ```\n\n2. **sshproxy:** If you have configured `sshproxy`, you can use the following command:\n\n   ```\n   ssh -i ~/.ssh/nersc <user>@perlmutter.nersc.gov   # or 'ssh -i ~/.ssh/nersc <user>@saul.nersc.gov'\n   ```\n\n   This assumes your identity file is in `~/.ssh/nersc`. The `sshproxy` route is convenient if you have multiple SSH connections without having to authenticate every time.\n\n3. **PuTTY (Windows):** If you are using Windows, you can use PuTTY to connect to Perlmutter. Here's how:\n\n   - Enter `myusername@perlmutter.nersc.gov` in the 'Host Name (or IP address)' field.\n   - After selecting the 'Connection' category and 'Credentials' from the 'Auth' submenu, click the 'Browse...' button in the 'Private key file for authentication' field to select a PPK file.\n   - Save the configuration and load it next time you want to connect to Perlmutter.\n\n4. **NoMachine (Windows):** If you prefer using NoMachine, follow these steps:\n\n   - Click the yellow Connect button at the top right. Enter your NERSC username.\n   - Verify the key and create a new desktop. You should land at the NERSC homescreen.\n   - Connect to Perlmutter via the green buttons on the left-hand menu.\n\n5. **Jupyter Notebooks (Interactive Visualization and Machine Learning):** For interactive notebooks well-suited for visualization and machine learning tasks, you can use Jupyter.\n\n6. **Python Jobs:** For running Python jobs at NERSC, you can use the following methods:\n\n   - Login nodes (for very small testing and debugging)\n   - Jupyter for interactive notebooks\n   - CPU or GPU compute nodes for substantial computation (interactively or via batch job)\n\nFor more detailed information, refer to the provided sources.\n\n\n\nReferences:\n * <https://docs.nersc.gov/connect/#connecting-with-ssh>\n * <https://docs.nersc.gov/development/languages/python/python-intro/#how-to-run-python-jobs-at-nersc>",
    "10": "To install and use a library that currently does not exist on Perlmutter, you can follow these general steps:\n\n1. **Determine the library requirements**: Understand the dependencies and system requirements of the library you want to install. This information can typically be found in the library's documentation or README file.\n\n2. **Choose an installation method**: NERSC provides several methods to install software, such as Spack, Conda, or manual installation from source. Choose the method that best suits your needs and is compatible with the library you want to install.\n\n3. **Install the library**: Once you have determined the installation method, follow the specific instructions for that method. For example, if you choose to use Spack, you can install the library by running `spack install <library-name>`. If you choose to install from source, you will need to download the source code, configure the build process, compile the code, and install the library.\n\n4. **Set up the environment**: After installing the library, you may need to set up your environment to use the library. This can include setting environment variables, loading modules, or adding the library to your system's search path.\n\n5. **Test the library**: After setting up your environment, test the library to ensure it is installed and functioning correctly. This can involve running sample programs, testing specific features, or running benchmarks.\n\n6. **Document your work**: Document the steps you took to install and use the library, including the installation method, any dependencies, and any specific configuration settings. This documentation can be helpful for yourself and others who may want to use the library in the future.\n\nHere are some specific examples of installing and using libraries on Perlmutter:\n\n- **Spack**: To install a library using Spack, follow the instructions in the NERSC documentation on Spack environments (<https://docs.nersc.gov/applications/e4s/spack_environments/>). Once installed, you can load the library using the `module load` command.\n\n- **Conda**: To install a library using Conda, follow the instructions in the NERSC documentation on using Conda for Python on Perlmutter (<https://docs.nersc.gov/development/languages/python/using-python-perlmutter/>). Once installed, you can load the library using the `module load` command.\n\n- **Manual installation**: If you need to install a library from source, follow the instructions in the library's documentation or README file. For example, to install the Jasper external library for WRF, you can follow the instructions in the NERSC documentation on WRF/WPS (<https://docs.nersc.gov/applications/wrf/wps/>). Once installed, you can load the library using the `module load` command.\n\nRemember to always follow NERSC's guidelines and best practices when installing and using software on their systems.\n\n\n\nReferences:\n * <https://docs.nersc.gov/>\n * <https://www.nersc.gov/users/getting-help/online-help-desk/>",
    "11": "To view your active and previous job statuses at NERSC, you can use the `squeue` command for active jobs and the `sacct` command for previous jobs. Here's a brief overview of how to use these commands:\n\n1. **Active Jobs (squeue):** The `squeue` command allows you to view the status of your active jobs on the NERSC supercomputer. To view all your active jobs, you can use the following command:\n\n```bash\nsqueue --me -t RUNNING\n```\n\nTo view all pending jobs for your current user, you can use:\n\n```bash\nsqueue --me -t PENDING\n```\n\nTo view all pending jobs in a specific Quality of Service (QoS), such as `shared`, you can use:\n\n```bash\nsqueue -q shared -t PENDING\n```\n\nTo view all running jobs for your current user in a specific QoS, such as `shared`, you can use:\n\n```bash\nsqueue --me -q shared -t RUNNING\n```\n\n1. **Previous Jobs (sacct):** The `sacct` command allows you to view the status of your previous jobs on the NERSC supercomputer. To view all your previous jobs, you can use the following command:\n\n```bash\nsacct -u <your_username>\n```\n\nReplace `<your_username>` with your actual NERSC username. This command will display a list of all your previous jobs, along with their status, start and end times, and other relevant information.\n\nRemember to replace `<your_username>` with your actual NERSC username when using the `sacct` command.\n\nIn summary, to view your active jobs, you can use the `squeue` command with appropriate flags, and to view your previous jobs, you can use the `sacct` command followed by your username.\n\nReferences:\n * <https://docs.nersc.gov/jobs/monitoring/#squeue>",
    "12": "It's possible that your job is getting killed due to exceeding the allocated time or resources. Here are some common reasons and solutions:\n\n1. **Exceeding the allocated time**: Jobs that run longer than the allocated time can be killed by the system. To avoid this, you can request more time by adjusting the `--time` flag in your `sbatch` command. For example, if your current allocation is 24 hours and you need more time, you can increase it to 48 hours:\n\n   ```\n   #SBATCH --time=48:00:00\n   ```\n\n2. **Exceeding the allocated resources**: If your job requires more resources than allocated, it may be killed by the system. To allocate more resources, you can adjust the `--ntasks` and `--cpus-per-task` flags in your `sbatch` command. For example, if you need to allocate 256 CPU cores, you can use:\n\n   ```\n   #SBATCH --ntasks=256\n   #SBATCH --cpus-per-task=1\n   ```\n\n3. **System-wide resource limits**: NERSC may enforce system-wide resource limits to ensure fair allocation of resources among all users. If your job exceeds these limits, it may be killed. In this case, you can try to optimize your code or reduce the resource requirements of your job.\n\n4. **Checkpointing and restarting**: If your job is a long-running simulation, you can enable checkpointing and restarting to save the current state of your job periodically. This allows you to resume your job from the last checkpoint in case it gets killed. You can use the `mana` tool for checkpointing and restarting your job. Here's an example of how to use it in your `sbatch` command:\n\n   ```\n   #SBATCH --comment=48:00:00\n   #SBATCH --signal=B:USR1@300\n   #SBATCH --requeue\n   #SBATCH --open-mode=append\n\n   module load mana nersc_cr\n\n   #checkpointing and restarting jobs\n   if [[ $(restart_count) == 0 ]]; then\n       srun -n 64 mana_launch ./a.out &\n   elif [[ $(restart_count) > 0 ]] && [[ -e dmtcp_restart_script.sh ]]; then\n       srun -n 64 mana_restart &\n   else\n       echo \"Failed to restart the job, exit\"; exit\n   fi\n\n   #requeueing the job if remaining time >0\n   ckpt_command=ckpt_mana\n\n   wait\n   ```\n\n5. **Job scheduling**: Jobs are scheduled based on their priority, resource requirements, and estimated completion time. If your job has a low priority or long estimated completion time, it may not be scheduled promptly. You can try to optimize your code or reduce the resource requirements of your job to improve its scheduling priority.\n\nIf you've tried these solutions and your job is still getting killed, please provide more information about your job, such as the error messages, the `sbatch` command you're using, and any relevant code snippets. This will help us identify the root cause of the issue and provide a more tailored solution.\n\nReferences:\n * <https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu>\n * <https://docs.nersc.gov/jobs/troubleshooting/#troubleshooting-jobs>\n * <https://docs.nersc.gov/applications/vasp/examples/long-time.sh>",
    "13": "To cancel all of your jobs at NERSC, you can use the `scancel` command followed by the `-u` flag and your username. Here's an example of how to do it:\n\n```\nscancel -u $USER\n```\n\nPlease note that canceling too many jobs at once may cause a degradation of service, as `scancel` sends a remote procedure call to the Slurm daemon. It is recommended to cancel jobs by subset instead of all at once.\n\nHere are some additional tips for managing your jobs at NERSC:\n\n- To cancel a specific job, use the following command:\n\n  ```\n  scancel $JobID\n  ```\n\n- To change the time limit of a job, use the `scontrol` command:\n\n  ```\n  scontrol update jobid=$JobID timelimit=$new_timelimit\n  ```\n\n- To change the quality of service (QOS) of a job, use the `scontrol` command:\n\n  ```\n  scontrol update jobid=$JobID qos=$new_qos\n  ```\n\n- To change the account associated with a job, use the `scontrol` command:\n\n  ```\n  scontrol update jobid=$JobID account=$new_project_to_charge\n  ```\n\nRemember to always refer to the NERSC documentation for the most accurate and up-to-date information on managing your jobs at the supercomputing center.\n\n\n\nReferences:\n * <https://docs.nersc.gov/jobs/monitoring/#updating-jobs>",
    "14": "To optimize your code for CPU usage at NERSC, it's crucial to focus on vectorization and parallelization. Vectorization allows your code to process multiple data points with a single instruction, effectively reducing the time your code takes to run through large datasets. Parallelization, on the other hand, involves dividing your code into multiple tasks that can be processed simultaneously, maximizing the use of available CPU resources. Combining these two strategies can lead to significant improvements in your code's performance on NERSC systems.\n\nHere are some resources to help you get started with optimizing your code for CPU usage at NERSC:\n\n1. NERSC's guide on vectorization: <https://docs.nersc.gov/performance/vectorization>\n2. NERSC's guide on parallelization: <https://docs.nersc.gov/performance/parallelism>\n3. Sample job script for running VASP 5 on Perlmutter CPU nodes: <https://docs.nersc.gov/applications/vasp/examples/perlmutter-cpu-v5.sh>\n4. NERSC's guide on launching programs with Python: <https://docs.nersc.gov/development/languages/python/parallel-python/#launching-programs>\n5. NERSC's guide on creating a batch script for TaskFarmer: <https://docs.nersc.gov/jobs/workflow/taskfarmer/#step-3-create-a-batch-script-submittaskfarmersl>\n6. Sample job script for preparing the environment and submitting a batch job: <https://docs.nersc.gov/jobs/examples/prepare-env/prepare-env.sh>\n\nThese resources provide detailed information on how to optimize your code for CPU usage at NERSC, as well as step-by-step guides on creating and submitting batch scripts for your jobs.\n\n\n\nReferences:\n * <https://docs.nersc.gov/performance/parallelism>\n * <https://docs.nersc.gov/development/languages/python/parallel-python/#launching-programs>\n * <https://docs.nersc.gov/performance/vectorization>\n * <https://docs.nersc.gov/applications/vasp/examples/perlmutter-cpu-v5.sh>\n * <https://docs.nersc.gov/jobs/workflow/taskfarmer/#step-3-create-a-batch-script-submittaskfarmersl>\n * <https://docs.nersc.gov/jobs/examples/prepare-env/prepare-env.sh>",
    "15": "You can directly ssh into Perlmutter from your offsite location without the need for a VPN. Here's how you can do it:\n\n1. Open a terminal or command prompt on your local computer.\n2. Type the following command, replacing `<username>` with your NERSC username:\n\n```bash\nssh <username>@perlmutter.nersc.gov\n```\n\nThis command will connect you to one of Perlmutter's login nodes. If you have generated a temporary SSH key using `sshproxy`, the command will connect you without any additional authentication. If you haven't generated a temporary SSH key, SSH will challenge you for your Iris password as well as your one-time password (OTP).\n\nIt's important to note that you should use the most up-to-date versions of SSH clients available to ensure a secure connection.\n\nHere are some helpful resources for more information:\n\n- [Connecting with SSH](https://docs.nersc.gov/connect/#connecting-with-ssh)\n- [sshproxy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy)\n\nRemember to always follow NERSC's guidelines and best practices for secure access and usage of their systems.\n\nReferences:\n * <https://docs.nersc.gov/connect/#connecting-with-ssh>\n * <https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy>",
    "16": "If your project runs out of time, your project will be restricted to the \"overrun\" and \"xfer\" queues. For example, if your project `m0001` has a negative balance since it used 75,000 node hours while its allocation is only 50,000 node hours, the project will be restricted to a subset of queues. If you have previously run jobs on default queues (`debug`, `regular`), your jobs will be stuck indefinitely, and you should consider killing them.\n\nTo resume running jobs, you can submit them to the \"overrun\" queue by using the `-q overrun` flag at job submission. On Perlmutter, all overrun jobs require the `--time-min` flag at job submission and are subject to preemption by higher priority workloads under certain circumstances. It is recommended to implement checkpoint/restart your overrun jobs to save your progress.\n\nHere's an example of how to submit a job to the overrun queue:\n```\nsbatch -q overrun --time-min=01:30:00 my_batch_script.sl\n```\n\nFor more information on managing overrun jobs, you can refer to the NERSC documentation on [projects that have exhausted their allocation](https://docs.nersc.gov/jobs/examples/#projects-that-have-exhausted-their-allocation) and [checkpoint/restart](https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu).\n\n\n\nReferences:\n * <https://docs.nersc.gov/jobs/examples/#projects-that-have-exhausted-their-allocation>\n * <https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu>\n * <https://docs.nersc.gov/jobs/troubleshooting/#troubleshooting-jobs>",
    "17": "To submit a job to Perlmutter, you can follow these steps:\n\n1. **Prepare a job script**: Write a job script that includes all the necessary commands and configurations for your application to run on Perlmutter. The script should specify the number of nodes, cores, and other resource requirements, as well as any specific settings for your application.\n\n2. **Submit the job**: Submit the job script to the Slurm scheduler using the `sbatch` command. For example, if your job script is named `my_job_script.sh`, you can submit it with the following command:\n\n   ```\n   nersc$ sbatch my_job_script.sh\n   ```\n\n3. **Check the job status**: After submitting the job, you can check its status using the `squeue` command. For example, to check the status of all your jobs, you can use:\n\n   ```\n   nersc$ squeue -u <your_username>\n   ```\n\n   Replace `<your_username>` with your actual username. This command will display a list of all your jobs along with their statuses.\n\n4. **Monitor the job progress**: While your job is running, you can monitor its progress using the `scontrol` command. For example, to show the details of a specific job, you can use:\n\n   ```\n   nersc$ scontrol show job <job_id>\n   ```\n\n   Replace `<job_id>` with the actual ID of your job. This command will display detailed information about the job, including its status, resource usage, and more.\n\n5. **Retrieve the job output**: Once the job is completed, you can retrieve its output files using the `scp` command. For example, to copy the output file `output.txt` from the Perlmutter system to your local machine, you can use:\n\n   ```\n   nersc$ scp <username>@perlmutter:/path/to/output/output.txt /local/path/to/save\n   ```\n\n   Replace `<username>`, `/path/to/output/`, and `/local/path/to/save` with the actual values for your job.\n\nBy following these steps, you can successfully submit a job to the Perlmutter supercomputer at NERSC and monitor its progress and output.\n\n\n\nReferences:\n * <https://docs.nersc.gov/services/sfapi/examples/#batch-job-submission>\n * <https://docs.nersc.gov/tutorials/playbooks/running/#step-4-write-a-batch-script>\n * <https://docs.nersc.gov/performance/vectorization>\n * <https://docs.nersc.gov/applications/vasp/examples/long-time.sh>",
    "18": "To set up Multi-Factor Authentication (MFA) at NERSC, follow these steps:\n\n1. Click the '**MFA not working?**' link on the [Iris login page](https://iris.nersc.gov).\n\n2. Enter your username and password. Click OK.\n\n3. A dialog box shows up and asks if you want to create a MFA token. Click OK.\n\n4. A new dialog box will show up. If you have entered the correct password, NERSC will send an email. The email contains a single-use one-time password (OTP).\n\n5. Use the OTP to log into Iris. Once logged in, [create and install a new MFA token](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/#creating-and-installing-a-token). Make sure to delete any previous MFA tokens that are no longer needed.\n\nFor more detailed information, refer to the NERSC documentation on [Configuring and Using an MFA Token](https://docs.nersc.gov/connect/mfa/#configuring-and-using-an-mfa-token).\n\n\n\nReferences:\n * <https://docs.nersc.gov/connect/mfa/#configuring-and-using-an-mfa-token>\n * <https://docs.nersc.gov/connect/mfa/#if-you-lost-your-tokens>",
    "19": "There are several reasons why you might not be able to log in to the NERSC supercomputing center:\n\n1. **Login Failures:** If you have ten login failures while entering your password on a NERSC resource, your login privileges will be disabled. However, this will automatically clear after 5 minutes, and no additional actions are necessary.\n\n2. **IP Address Issues:** Web services in Spin are made accessible externally by an _ingress controller_, which is a web proxy. Most web services, by default, will therefore log accesses as originating from the IP address of the ingress controller rather than that of the user's workstation. To change this behavior, configure your web service to obtain the IP address from the `X-Forwarded-For` HTTP header, which is a convention for passing this information through web proxies. The ingress controller in Spin is already configured to populate the IP address into the `X-Forwarded-For` HTTP header.\n\n3. **Connection Failures:** If you are having trouble connecting to NoMachine, try the following steps to troubleshoot the issue:\n\n   a. Log into [Iris](https://iris.nersc.gov) to clear any login failures. Access to NoMachine uses your NERSC user name and password. If your password is mistyped five times, NERSC will lock you out of its systems. Logging into Iris will automatically clear these failures. This will also let you know if your password is expired (which would prevent you from accessing NoMachine, among many other things).\n\n   b. Create a new connection file following the instructions provided in the NERSC documentation. NX will often \"update\" the configuration file to try to save your settings, and occasionally incorrect settings can be saved. You must have the new NoMachine player AND an updated configuration file to connect to the NoMachine service.\n\n   c. Try to ssh directly to the NoMachine server. This will help to distinguish if you are having a connection issue that is specific to NoMachine or a general SSH connection issue. You can do this with the command `ssh <nersc_username>@nxcloud01.nersc.gov` and your NERSC user name and password+one-time MFA password (with no spaces in between). If your access to the NoMachine server is blocked by a local firewall or something else and you can't access it, you may need to check your network settings or contact your network administrator for assistance.\n\n4. **MFA Timeouts:** If you have enabled Multi-Factor Authentication (MFA) and are experiencing login failures repeatedly, you can follow these steps to resolve the issue:\n\n   a. If this is with a particular host (e.g., Perlmutter) only, log in to your Iris account and select the 'Profile' tab. Click the 'Account Locked?' button, and click the 'Unlock Account' button in the dialog box that will appear. This will clear login failures that may have accumulated for the host. Then, try to log in to the host again.\n\n   b. If you enter incorrect One-Time Passwords (OTPs) too many times, the NERSC MFA server locks you out. In this case, you have to wait for 15 minutes before you try again.\n\n5. **Ssh Key Authentication Issues:** If you are using ssh key authentication via `sshproxy.sh` for authentication, check if the keys have expired. A popular way of using ssh key authentication is via ssh-agent, the \"authentication agent.\" You add an ssh private key to ssh-agent and it uses the key to authenticate to a remote host that has the matching public key. You may be knowingly or unknowingly using this method (especially, when you use the `-a` option with `sshproxy.sh`). Ssh-agent goes through the saved keys one by one to see if the correct key is found. If it cannot find the matching key within 6 tries, ssh authentication fails. When you have many keys stored in ssh-agent, including the correct one, login can fail if the correct key is not selected within the first 6 tries. To see how many keys are stored in `ssh-agent`, run the command `ssh-add -l` on your laptop/desktop. If you see many keys there, you can delete all of them to resolve the issue.\n\n6. **MPI4Py Timeouts or Slowness:** Running `mpi4py` on a large number of nodes can become slow due to all the metadata that must move across our filesystems. You may experience timeouts that look like this:\n\n```shell\nsrun: job 33116771 has been allocated resources\nMon Aug 3 18:24:50 2020: [PE_224]:inet_connect:inet_connect: connect failed after 301 attempts\nMon Aug 3 18:24:50 2020: [PE_224]:_pmi_inet_setup:inet_connect failed\nMon Aug 3 18:24:50 2020: [PE_224]:_pmi_init:_pmi_inet_setup (full) returned -1\n[Mon Aug 3 18:24:50 2020] [c0-0c2s7n1] Fatal error in PMPI_Init_thread: Other MPI error, error stack:\nMPIR_Init_thread(537):\nMPID_Init(246).......: channel initialization failed\nMPID_Init(647).......: PMI2 init failed: 1\n```\n\n*Easy (but temporary) fix:*\n\n```shell\nexport PMI_MMAP_SYNC_WAIT_TIME=300\n```\n\nHowever, this doesn't fix the problem, it just gives you more time to start up.\n\nIf you are still unable to log in after trying these troubleshooting steps, please contact the NERSC Account Support Office by sending an email to accounts@nersc.gov for further assistance.\n\nReferences:\n * <https://docs.nersc.gov/development/languages/python/faq-troubleshooting/#why-does-my-mpi4py-time-out-or-why-is-it-so-slow>\n * <https://docs.nersc.gov/iris/iris-for-users/#problems-accessing-iris>\n * <https://docs.nersc.gov/services/spin/faq/#why-are-ip-addresses-in-the-10420016-range-showing-in-my-web-service-access-log>\n * <https://docs.nersc.gov/connect/nx/#troubleshooting-nx-connection-failures>\n * <https://docs.nersc.gov/accounts/passwords/#login-failures>\n * <https://docs.nersc.gov/connect/mfa/#q-i-have-enabled-mfa-my-logins-fail-repeatedly-what-should-i-do>",
    "20": "When deciding which file system to use for storing your data at NERSC, it's important to consider the purpose and performance of each file system. Here are some general guidelines:\n\n1. **Home**: This file system is designed for user-specific data and is accessible by individual users. It's a good choice for storing small amounts of data that you need quick access to. However, it's not recommended for large datasets or long-term storage.\n\n2. **Common**: This file system is designed for project-specific data and is accessible by project members. It's a good choice for storing larger datasets that need to be shared among a group of users. However, it's not recommended for long-term storage or large datasets that require high-performance access.\n\n3. **Community**: This file system is also designed for project-specific data and is accessible by project members. It offers snapshots and is a good choice for storing large datasets that need to be shared among a group of users. It's also designed to be accessed and used either by a user individually or by their project, as reported in the \"Access\" column.\n\n4. **Perlmutter Scratch**: This file system is designed for high-performance computing and is accessible by user-specific data. It's a good choice for storing large datasets that require high-performance access. However, it's not recommended for long-term storage or small amounts of data.\n\n5. **HPSS**: This file system is designed for long-term storage of data that is not frequently accessed. It's a good choice for storing large datasets that need to be stored for an extended period but are not accessed frequently. However, it's not recommended for small amounts of data or high-performance access.\n\nIn summary, the choice of file system depends on the size of your dataset, the frequency of access, and the performance requirements of your workload. For example, if you have a large dataset that requires high-performance access, you should use the Perlmutter Scratch file system. If you have a small dataset that needs to be accessed frequently, you should use the Home file system.\n\n\n\nReferences:\n * <https://docs.nersc.gov/filesystems/#summary>\n * <https://docs.nersc.gov/performance/io/#best-practices-for-scientific-io>\n * <https://docs.nersc.gov/performance/io/dvs/#read-your-data-from-the-right-place>\n * <https://docs.nersc.gov/filesystems/#local-storage>"
}